{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z [[94638.78184676]]\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "error 9123360941.633387\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def dataset(data):\n",
    "    random.seed(32)\n",
    "    data_ = list(zip( data['idx'], data['forecasted_performance'],data['hs_sleep'], data['hs_study'], data['prev_scores'], data['practice'], data['extracurricular_activities']))\n",
    "    random.shuffle(data_)\n",
    "    idx, y, x1, x2, x3, x4, x5 = zip(*data_)\n",
    "    x1 = list(x1)\n",
    "    x2 = list(x2)\n",
    "    x3 = list(x3)\n",
    "    x4 = list(x4)\n",
    "    x5= list(x5)\n",
    "    y = list(y)\n",
    "    for i in range(len(x5)):\n",
    "        if x5[i] == False:\n",
    "            x5[i] = 0\n",
    "        else:\n",
    "            x5[i] = 1\n",
    "\n",
    "    X = np.zeros((len(x1), 5))\n",
    "    for i in range(len(X)):\n",
    "        X[i][0] = x1[i]\n",
    "        X[i][1] = x2[i]\n",
    "        X[i][2] = x3[i]\n",
    "        X[i][3] = x4[i]\n",
    "        X[i][4] = x5[i]\n",
    "    \n",
    "    Y = np.zeros((len(x1), 1))\n",
    "    for i in range(len(Y)):\n",
    "        Y[i] = y[i]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layers_, activation_functions, activation_grads_):\n",
    "        np.random.seed(42)\n",
    "        self.layers = layers_\n",
    "        self.W = [np.random.normal(0, 0.2, (layers_[i+1], layers_[i])) for i in range(len(layers_)-1)]\n",
    "        self.B = [np.random.normal(0, 0.2, (layers_[i], 1)) for i in range(1, len(layers_))]\n",
    "        self.activations = activation_functions\n",
    "        self.activation_grads = activation_grads_\n",
    "        \n",
    "\n",
    "    def evaluate(self, X):\n",
    "        values_a = [np.zeros((self.layers[i], 1)) for i in range(len(self.layers))]\n",
    "        values_z = [np.zeros((self.layers[i], 1)) for i in range(len(self.layers))]\n",
    "\n",
    "        for layer in range(len(values_z)):\n",
    "            if layer == 0:\n",
    "                for neuron in range(len(values_z[layer])):\n",
    "                    values_z[layer][neuron] = X[neuron]\n",
    "            else:\n",
    "                values_a[layer] = self.W[layer-1] @ values_z[layer-1] + self.B[layer-1]\n",
    "                values_z[layer] = self.activations[layer-1](np.copy(values_a[layer]))\n",
    "        return values_a, values_z\n",
    "\n",
    "    def backpropagation(self, values_a, values_z, Y, loss_grad):\n",
    "        W_grad = [np.zeros((len(self.W[i]), len(self.W[i][0]))) for i in range(len(self.W))]\n",
    "        B_grad = [np.zeros((len(self.B[i]), 1)) for i in range(len(self.B))]\n",
    "        delta = [np.zeros((len(values_z[i]), 1)) for i in range(len(values_z))]\n",
    "        err = loss_grad(Y, values_z[-1])  \n",
    "        delta[-1] = err @ self.activation_grads[-1](values_a[-1])\n",
    "        \n",
    "        W_grad[-1] = delta[-1] @ np.transpose(values_z[-2]) \n",
    "        B_grad[-1] = delta[-1]\n",
    "        for layer in range(len(self.layers)-1, 1, -1):\n",
    "            delta[layer-1] =  np.diag((self.activation_grads[layer-1](values_a[layer-1])).flatten()) @ np.transpose(self.W[layer-1]) @ delta[layer]\n",
    "            W_grad[layer - 2] = delta[layer-1] @ np.transpose(values_z[layer-2])\n",
    "            B_grad[layer-2] = delta[layer-1]\n",
    "        return W_grad, B_grad\n",
    "    \n",
    "    def gradient_descent(self, data_train, learning_rate, epochs): \n",
    "        for epoch in range(epochs):\n",
    "            W_grad_total = [np.zeros_like(w) for w in self.W]\n",
    "            B_grad_total = [np.zeros_like(b) for b in self.B]\n",
    "            for X, Y in data_train:\n",
    "                a, z  = self.evaluate(X)\n",
    "                W_grad, B_grad = self.backpropagation(a, z, Y, MSE_grad)\n",
    "                for i in range(len(W_grad_total)):\n",
    "                    W_grad_total[i] += W_grad[i]\n",
    "                    B_grad_total[i] += B_grad[i]\n",
    "            \n",
    "            for layer in range(len(self.W)):\n",
    "                self.W[layer] -= learning_rate * (W_grad_total[layer]/len(data_train))\n",
    "                self.B[layer] -= learning_rate * B_grad_total[layer]\n",
    "\n",
    "\n",
    "    def stochastic_gradient_descent(self, data_train, learning_rate, epochs):\n",
    "        for epoch in epochs:\n",
    "            for X, Y in data_train:\n",
    "                a, z  = self.evaluate(X)\n",
    "                W_grad, B_grad = self.backpropagation(a, z, Y, MSE_grad)\n",
    "                for layer in range(len(self.W)):\n",
    "                    self.W[layer] -= learning_rate * W_grad[layer]\n",
    "                    self.B[layer] -= learning_rate * B_grad[layer]\n",
    "\n",
    "\n",
    "    \n",
    "    def mini_baches(self, data_train, learning_rate, epochs, batch_size):\n",
    "        for epoch in epochs:\n",
    "            np.random.shuffle(data_train)\n",
    "            mini_batches = [data_train[k:k+batch_size] for k in range(0, len(data_train), batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                W_grad_total = [np.zeros_like(w) for w in self.W]\n",
    "                B_grad_total = [np.zeros_like(b) for b in self.B]\n",
    "                for X, Y in mini_batch:\n",
    "                    a, z  = self.evaluate(X)\n",
    "                    W_grad, B_grad = self.backpropagation(a, z, Y, MSE_grad)\n",
    "                    for i in range(len(W_grad_total)):\n",
    "                        W_grad_total[i] += W_grad[i]\n",
    "                        B_grad_total[i] += B_grad[i]\n",
    "                \n",
    "                for layer in range(len(self.W)):\n",
    "                    self.W[layer] -= learning_rate * W_grad_total[layer]\n",
    "                    self.B[layer] -= learning_rate * B_grad_total[layer]\n",
    "\n",
    "\n",
    "\n",
    "def relu(value):\n",
    "    for i in range(len(value)):\n",
    "        value[i] = max(0, value[i])\n",
    "    return  value\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def MSE_grad(real_y, predict):\n",
    "    return 2*(predict - real_y)\n",
    "\n",
    "\n",
    "def relu_grad(x):\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "\n",
    "def linear_grad(x):\n",
    "    return np.ones_like(x)\n",
    "\n",
    "def error(data, predict):\n",
    "    return np.mean((data - predict)**2)\n",
    "\n",
    "def main():\n",
    "    path = 'datasets/Student_Performance_DEV.csv'\n",
    "    y_predict = []\n",
    "    data = pd.read_csv(path)\n",
    "    X, Y = dataset(data)\n",
    "    NN = Network([5, 2, 1], [relu, linear], [relu_grad, linear_grad])\n",
    "    data_train = [[X[i], Y[i]] for i in range(int(0.8*len(X)))]\n",
    "    NN.gradient_descent(data_train, 0.01, 3)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(int(0.8 * len(X)), len(X)):\n",
    "        y_predict.append(NN.evaluate(X[i])[1][-1][0])\n",
    "    err = error(Y[int(0.8 * len(X)):], y_predict)\n",
    "\n",
    "    print(\"error\", err)\n",
    "    \n",
    "        \n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
